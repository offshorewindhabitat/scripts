---
title: "equation"
format: html
editor_options: 
  chunk_output_type: console
---


$$
rescale(x) = [x - min(x)] / [max(x) - min(x)] \\
\\
Score_{raw} = w_{SR} * rescale(SpeciesRichness) \\
+ w_{SA} * rescale(SpeciesAbundance) \\
+ w_{ER} * rescale(ExtinctionRisk) \\
+ w_{PP} * rescale(PrimaryProductivity) \\
+ w_{HV} * rescale(HydrothermalVents) \\
+ w_{SM} * rescale(Seamounts) \\
Score = rescale(Score_{raw}) * 100
$$
```{r pkgs & db con}
library(librarian)
shelf(
  DBI, devtools, dplyr, fs, glue, here, readr, scales, stringr, terra)
load_all("../offhabr")

dir_derived <- "/Users/bbest/My Drive/projects/offhab/data/derived"
dir_raw     <- "/Users/bbest/My Drive/projects/offhab/data/raw"

con <- oh_con(read_only = F) # dbDisconnect(con, shutdown = T)
# dbListTables(con)
```

```{r get non-aphia lyrs}
d_lyrs <- tbl(con, "lyrs") |> 
  filter(
    is_ds_prime,
    is.na(aphia_id)) |> 
  collect()
# View(d_lyrs)

if ("nc" %in% d_lyrs$lyr_key){
  r_nc <- d_lyrs |> 
    filter(lyr_key == "nc") |> 
    pull(path_tif) |> 
    rast() |> 
    trim()
  # plot(r_nc)
  # plet(r_nc), providers$Esri.OceanBasemap)
  
  dbExecute(con, "DELETE FROM lyrs WHERE lyr_key == 'nc'")
}
```


```{r prep lyrs}
# paths ----
lyr_pfxs     <- c("sa", "sp", "er", "vg", "ve", "sm")
lyr_web_tifs <- setNames(
  glue("{dir_derived}/{lyr_pfxs}_web.tif"), lyr_pfxs)
lyr_score_v1_web_tif <- glue("{dir_derived}/score_v1_web.tif")

# weights ----
w <- list(
  sa = 1,
  sp = 1,
  er = 1,
  vg = 1,
  ve = 1,
  sm = 1)

# read layers into raster stack, create rasters if missing ----
stk <- list()
for (lyr_pfx in lyr_pfxs){ # lyr_pfx = lyr_pfxs[3] # lyr_pfx = "er"
  
  lyr_web_tif <- lyr_web_tifs[lyr_pfx]
  
  if (!file.exists(lyr_web_tif)){
    message(glue("Creating: {basename(lyr_web_tif)}"))
    
    lyr_tif <- switch(
      lyr_pfx,
      er = glue("{dir_derived}/er.tif"),
      vg = glue("{dir_raw}/oregonstate.edu - productivity/vgpm.m.2021/vgpm_2021_oh.tif"),
      d_lyrs |> 
        filter(lyr_key == lyr_pfx) |> 
        pull(path_tif) |> as.character())
    message(glue("  lyr_tif: {basename(lyr_tif)}"))
    
    r_lyr <- rast(lyr_tif)
    # dimensions  : 7183, 14678, 1  (nrow, ncol, nlyr)
    # resolution  : 481.3177, 481.3177  (x, y)
    # extent      : -14378304, -7313523, 2733139, 6190444
    (rng_lyr <- range(values(r_lyr, na.rm=T))) # er: 150 to 22,585
    message(glue("  range: {paste(rng_lyr, collapse = ',')}"))

    write_rast(
      r_lyr, lyr_web_tif,
      web_optimize = T)

    r_lyr_web <- rast(lyr_web_tif)
    # dimensions  : 5888, 11776, 1  (nrow, ncol, nlyr)
    # resolution  : 611.4962, 611.4962  (x, y)
    # extent      : -14401959, -7200980, 2661232, 6261721
    if (lyr_pfx != "sa")
      stopifnot(compareGeom(r_lyr_web, stk[["sa"]]))
  }
  stk[[lyr_pfx]] <- rast(lyr_web_tif)
}
stk <- rast(stk)

# calculate score ----
# names(stk)
# "sa" "sp" "er" "vg" "ve" "sm"

sc <- function(x){
  terra::setValues(x, scales::rescale(values(x)))
}
r_sa   <- sc(stk["sa"])
r_sp   <- sc(stk["sp"])
r_er   <- sc(stk["er"])
r_vg   <- sc(stk["vg"])
r_ve   <- sc(stk["ve"]) # HydrothermalVents: binary converts to 0.5
r_sm   <- sc(stk["sm"]) # Seamounts:         binary converts to 0.5

# score_v1: all weights == 1
r_score_v1 <- sc(sum(r_sa, r_sp, r_er, r_vg, r_ve, r_sm, na.rm = T)) * 100 |> as.int()
# plet(r_score_v1, tiles=providers$CartoDB.Positron)
write_rast(
  r_score_v1,
  lyr_score_v1_web_tif,
  use_gdal_cog = T)

# write rescaled input lyrs for evaluating contribution of each per pixel ----

for (lyr_pfx in lyr_pfxs){ # lyr_pfx = lyr_pfxs[1]
  r   <- get(glue("r_{lyr_pfx}"))
  tif <- glue("{dir_derived}/{lyr_pfx}_web_rescaled.tif")
  r <- (r * 100) |> as.int()
  write_rast(r, tif, use_gdal_cog = T)
}
lyr_web_rescaled_tifs <- setNames(
  glue("{dir_derived}/{lyr_pfxs}_web_rescaled.tif"), lyr_pfxs)


# upload to gcs ----
tifs_for_gcs <- c(
  lyr_web_tifs, lyr_score_v1_web_tif, lyr_web_rescaled_tifs)
overwrite_gcs = F

# setup GCS
Sys.setenv(
  "GCS_DEFAULT_BUCKET" = "offhab_lyrs",
  "GCS_AUTH_FILE"      = "/Users/bbest/My Drive/private/offhab-google-service-account_09e7228ac965.json")
library(googleCloudStorageR)

if (!overwrite_gcs){
  gcs_objects <- gcs_list_objects()

  gcs_missing <- setdiff(basename(tifs_for_gcs), gcs_objects$name)
  tifs_for_gcs <- glue("{dir_derived}/{gcs_missing}")
}
sapply(tifs_for_gcs, upload_to_gcs)
```

```{r get zonal stats on all lyrs for blocks zones regions & study}
web_tifs <- fs::dir_ls(dir_derived, glob="*web*.tif")
s_web <- rast(web_tifs)
names(s_web) <- basename(web_tifs) |> path_ext_remove()

r_b <- oh_rast(type = "block_id", zone_version = 1, web_version = T)
d_b <- zonal(s_web, r_b, "mean", na.rm=T) |> tibble()
write_csv(d_b, here("data/zonal_blocks.csv"))

r_z <- oh_rast(type = "zone_id", zone_version = 1, web_version = T)
d_z <- zonal(s_web, r_z, "mean", na.rm=T) |> tibble()
write_csv(d_z, here("data/zonal_zones.csv"))

r_r <- oh_rast(type = "region", zone_version = 1, web_version = T)
d_r <- zonal(s_web, r_r, "mean", na.rm=T) |> tibble()
write_csv(d_r, here("data/zonal_regions.csv"))

d_g <- global(s_web, "mean", na.rm=T) |> tibble()
write_csv(d_r, here("data/zonal_study.csv"))
```

```{r get min/max into db for all layers to oh_map_cog()}
d_lyrs_notaphia <- tbl(con, "lyrs") |> 
  filter(is.na(aphia_id)) |> 
  collect()
# View(d_lyrs_notaphia)

for (lyr_key in names(s_web)){ # lyr_key = names(s_web)[2]
  
  path_tif = glue("{dir_derived}/{lyr_key}.tif")
  stopifnot(file.exists(path_tif))
  
  r   <- rast(path_tif)
  rng <- range(values(r, na.rm=T))
  
  message(glue("{lyr_key} [{paste(rng, collapse=',')}] "))
  
  d <- tibble(
    path_tif    = path_tif,
    lyr_key     = lyr_key,
    ds_key      = lyr_key |> 
      str_remove("_web") |> 
      str_remove("_rescaled"),
    val_min     = rng[1],
    val_max     = rng[2],
    is_ds_prime = TRUE)
  
  dbExecute(con, "DELETE FROM lyrs WHERE lyr_key == '{lyr_key}'")
  DBI::dbAppendTable(con, "lyrs", d)
}
```

```{r add taxa_wm info where missing aphia_id}
d_aphias_missing <- tbl(con, "lyrs") |> 
  filter(
    !is.na(aphia_id),
    is_ds_prime) |> 
  select(aphia_id) |> 
  anti_join(
    tbl(con, "taxa_wm"),
    by = "aphia_id") |> 
  collect() |> 
  wm_rest(aphia_id, operation = "AphiaRecordsByAphiaIDs")

# check not missing any fields or values
flds_d  <- colnames(d_aphias_missing)
flds_db <- colnames(tbl(con, "taxa_wm"))
stopifnot(all(
  length(setdiff(flds_d, flds_db)) == 0,
  length(setdiff(flds_db, flds_d)) == 0),
  sum(is.na(d_aphias_missing$kingdom)))

dbAppendTable(
  con, "taxa_wm",
  d_aphias_missing)
```


```{r test web.tif and tileserver expressions by zone}
stk_web_tif <- path(dir_derived, "stack_web.tif")

d_stk_web <- tibble(
  path_tif   = c(
    # all derived rasters, except stack_web.tif
    setdiff(
      dir_ls(dir_derived, glob="*web*.tif"),
      stk_web_tif),
    # web rasters in offhabr/inst folder (except oh_blocks_v1_web.tif)
    #   dir_ls(here("../offhabr/inst"), glob="*.tif") |> path_file() |> path_ext_remove()
    sprintf(
      "%s/%s.tif", here('../offhabr/inst'), c(
      "oh_regions_web", "oh_zones_v1_web", "oh_blocks_v1_web", "oh_elev_m_web", "oh_area_m2_web"))),
  tif_exists = file_exists(path_tif),
  lyr_key    = path_file(path_tif) |> path_ext_remove()) |> 
  arrange(lyr_key)
stopifnot(all(d_stk_web$tif_exists))

# construct raster stack
stk_web <- rast(d_stk_web$path_tif)

# add NA layer
r_na <- setValues(stk_web[[1]], NA)
stk_web <- c(stk_web, r_na)
names(stk_web) <- c(d_stk_web$lyr_key, "NA")
# which(names(stk_web)=="score_v1_web") # 10

# write raster
write_rast(
  stk_web, stk_web_tif, 
  datatype     = "FLT4S",
  use_gdal_cog = TRUE, )

# setup GCS
Sys.setenv(
  "GCS_DEFAULT_BUCKET" = "offhab_lyrs",
  "GCS_AUTH_FILE"      = "/Users/bbest/My Drive/private/offhab-google-service-account_09e7228ac965.json")
library(googleCloudStorageR)
upload_to_gcs(stk_web_tif)

# https://api.cogeo.xyz/cog/tiles/WebMercatorQuad/9/85/205@2x?url=https://storage.googleapis.com/offhab_lyrs/stack_web.tif&resampling_method=average&rescale=1,100&return_mask=true&colormap_name=spectral_r&expression=where(oh_zones_v1_web==1,score_v1_web,NA)


# offhabr::oh_zones_s1k |> sf::st_drop_geometry() |> slice(1) |> select(1:5)
#   zone_id zone_version zone_key zone_name          region 
#     <int>        <dbl> <glue>   <glue>             <chr>  
#         1            1 cec      Central California Pacific

cog_file       = path_file(stk_web_tif)
cog_dir        = "https://storage.googleapis.com/offhab_lyrs"
cog_range      = stk_web[["score_v1_web"]] |> values(na.rm=T) |> range() # c(1, 100)
cog_method     = "average"
cog_palette    = "spectral_r"
lgnd_palette   = "Spectral"
lgnd_palette_r = TRUE
bb             = c(-129.4, 23.2, -64.7, 48.9)
title          = "% Habitat"
base_opacity   = 0.5

cog_url   <- glue("{cog_dir}/{cog_file}")


# tile_opts <- glue(
#   "resampling_method={cog_method}&rescale={paste(cog_range, collapse=',')}&return_mask=true&colormap_name={cog_palette}&bidx=10") # works

which(names(stk_web)=="oh_regions_web")  #  6
which(names(stk_web)=="oh_zones_v1_web") #  7
which(names(stk_web)=="score_v1_web")    # 10
which(names(stk_web)=="NA")              # 19
# bidx=7,10,19
# expression = "where(B07==1,B10,B19)" # doesnt' work

# range(values(stk_web[["oh_zones_v1_web"]], na.rm=T)) # 1,11
expression = "b7"; cog_range = c(1,11) # works

range(values(stk_web[["oh_regions_web"]], na.rm=T)) # 1,3
expression = "b6"; cog_range = c(1,3) # works
expression = "int(b6>1)"; cog_range = c(0,1) # works

# range(values(
#   stk_web[["oh_zones_v1_web"]] * stk_web[["score_v1_web"]], 
#   na.rm=T)) # 0,902
# expression = "b7+b10"; cog_range = c(0,902) # doesn't work

# expression = "B07+B10"

tile_opts <- glue(
  "resampling_method={cog_method}&rescale={paste(cog_range, collapse=',')}&return_mask=true&colormap_name={cog_palette}&expression={expression}") 

tile_url  <- glue(
  "https://api.cogeo.xyz/cog/tiles/WebMercatorQuad/{{z}}/{{x}}/{{y}}@2x?url={cog_url}&{tile_opts}")


leaflet::leaflet() |>
  # add base: blue bathymetry and light brown/green topography
  leaflet::addProviderTiles(
    "Esri.OceanBasemap",
    options = providerTileOptions(
      variant = "Ocean/World_Ocean_Base",
      opacity = base_opacity)) |>
  # add reference: placename labels and borders
  leaflet::addProviderTiles(
    "Esri.OceanBasemap",
    options = providerTileOptions(
      variant = "Ocean/World_Ocean_Reference",
      opacity = base_opacity)) |>
  addTiles(
    urlTemplate=tile_url) |>
  fitBounds(bb[1], bb[2], bb[3], bb[4]) |>
  leaflet.extras::addFullscreenControl() |>
addLegend(
  pal    = colorNumeric(lgnd_palette, cog_range[1]:cog_range[2], reverse = lgnd_palette_r),
  values = c(cog_range[1], cog_range[2]),
  title  = title)
```


```{r map zone with score and blocks}
librarian::shelf(
  sf)


# offhabr::oh_blocks |> 
#   st_drop_geometry() |> 
#   filter(zone_version == 1) |> 
#   pull(zone_key) |> 
#   table()
# cec  cgm  mda  noa  noc  soa  soc  wao  wgm 
#  16   33 2165  342  237  801  673  753   96 
zone_keys  <- oh_blocks |> 
  filter(zone_version == 1) |>  
  pull(zone_key) |> 
  unique()

zone_key <- "mda"
m <- oh_map_zone_score_dev(zone_key)
attr(m, "zone_name") # Mid Atlantic
m
```

```{r ensure all spp layers are on gcs}
Sys.setenv(
  "GCS_DEFAULT_BUCKET" = "offhab_lyrs",
  "GCS_AUTH_FILE"      = "/Users/bbest/My Drive/private/offhab-google-service-account_09e7228ac965.json")
library(googleCloudStorageR)

gcs_objects  <- gcs_list_objects()

d <- tbl(con, "lyrs") |> 
  filter(is_ds_prime) |> 
  collect() |> 
  mutate(
    tif         = path_file(path_tif),
    path_is_lyr = path_ext_remove(tif) == lyr_key,
    tif_on_gcs  = tif %in% gcs_objects$name)
stopifnot(sum(!d$path_is_lyr)==0)
table(d$tif_on_gcs)
# FALSE  TRUE 
#   137  9941

tifs_for_gcs <- d |> filter(!tif_on_gcs) |> pull(path_tif)
sapply(tifs_for_gcs, upload_to_gcs)
```

```{r update lyrs with val_min val_max bbox}
librarian::shelf(
  progress, sf, terra)

sql <- "
  UPDATE lyrs 
  SET 
    val_min = 1,
    val_max = 100
  WHERE 
    (val_min IS NULL OR 
     val_max IS NULL) AND
    aphia_id IS NOT NULL"
dbExecute(con, sql)

dbExecute(con, "ALTER TABLE lyrs DROP COLUMN bbox")
dbExecute(con, "ALTER TABLE lyrs ADD COLUMN bbox REAL[]")

d_lyrs <- tbl(con, "lyrs") |> 
  filter(is_ds_prime == TRUE) |> 
  collect()

pb <- progress_bar$new(
  format = " updating [:bar] :current/:total :percent in :elapsed; eta: :eta",
  total = nrow(d_lyrs))
for (i in 1:nrow(d_lyrs)){
  pb$tick()
  lyr_key  <- d_lyrs$lyr_key[i] # am_100832
  path_tif <- d_lyrs$path_tif[i]
  bb <- rast(path_tif) |> 
    trim() |> 
    ext() |>  # (xmin, xmax, ymin, ymax)
    st_bbox() |> # xmin      ymin      xmax      ymax
    st_as_sfc() |> 
    st_set_crs(3857) |> 
    st_transform(4326) |> 
    st_bbox() |> 
    as.numeric()
  # leaflet::fitBounds() # lng1, lat1, lng2, lat2
  
  sql <- glue("
  UPDATE lyrs 
  SET 
    bbox = ([{paste(bb, collapse=',')}]),
  WHERE 
    lyr_key     = '{lyr_key}' AND
    is_ds_prime = TRUE") # ; cat(sql)
  dbExecute(con, sql)
}
```

```{r update all lyrs to public in gcs}
Sys.setenv(
  "GCS_DEFAULT_BUCKET" = "offhab_lyrs",
  "GCS_AUTH_FILE"      = "/Users/bbest/My Drive/private/offhab-google-service-account_09e7228ac965.json")
library(googleCloudStorageR)

tifs <- tbl(con, "lyrs") |> 
  filter(is_ds_prime == T) |> 
  collect() |> 
  mutate(
    base_tif = basename(path_tif)) |> 
  pull(base_tif)

# make all layers public
sapply(tifs, gcs_update_object_acl, entity_type = "allUsers")
```

```{r dbDisconnect}
dbDisconnect(con, shutdown=T) # con <- oh_con(read_only = F)
```



